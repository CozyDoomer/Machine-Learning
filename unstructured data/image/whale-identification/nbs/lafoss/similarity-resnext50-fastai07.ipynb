{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### Overview\n",
    "The goal of this competition is identifying individual whales in images. Despite several whales are well represented in images, most of whales are unique or shown only in a few pictures. In particular, the train dataset includes 25k images and 5k unique whale ids. In addition, ~10k of images show unique whales ('new_whale' label). Checking public kernels suggests that a classical approach for classification problems based on softmax prediction for all classes is working quite well for this particular problem. However, strong class imbalance, handling labels represented by just several images, and 'new_whale' label deteriorates this approach. In addition, form the using this model for production, the above approach doesn't sound right since expansion of the model to identify new whales not represented in the train dataset would require retraining the model with increased softmax size. Meanwhile, the task of this competition could be reconsidered as checking similarities that suggests one-shot based learning algorithm to be applicable. This approach is less susceptible to data imbalance in this competition, can naturally handle 'new_whale' class, and is scalable in terms of a model for production (new classes can be added without retraining the model).\n",
    "\n",
    "There are several public kernels targeted at using similarity based approach. First of all, it is an amazing [kernel posted by Martin Piotte](https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563), which discusses Siamese Neural Network architecture in details. A [fork of this kernel](https://www.kaggle.com/seesee/siamese-pretrained-0-822/notebook) reports 0.822 public LB score after training for 400 epochs. There is also a quite interesting [public kernel](https://www.kaggle.com/ashishpatel26/triplet-loss-network-for-humpback-whale-prediction) discussing Triplet Neural Network architecture, which is supposed to overperform Siamese architecture (check links in [this discussion](https://www.kaggle.com/c/humpback-whale-identification/discussion/76012)). Since both positive and negative examples are provided, the gradients are appeared to be more stable, and the network is not only trying to get away from negative or get close to positive example but arranges the prediction to fulfil both.\n",
    "\n",
    "In this kernel I provide an example of a network inspired by Triplet architecture that is capable to reach **~0.60 public LB score after training only for 11 epochs**.  Training for more epochs is supposed to improve the prediction even further, and hopefully it will take much less than 400 epochs to reach 0.8+ public LB score (I'll post an update after I check it). The main trick of this kernel is **using multiple loss instead of triplet one**. If the forward pass is completed for all images in a batch, why shouldn't I compare all of them when calculate the loss function? why should I limit myself by just several triplets? I have designed a loss function in such a way that allows performing all vs. all comparison within each batch, in other words for a batch of size 16 instead of comparing 16 triplets or 32 pairs the network performs processing of 2256 pairs of images at the same time. If training is done on multiple GPUs, the number of compared pares could be boosted even further since it it proportional to bs^2. Such a huge number of processed pairs further stabilizes gradients in comparison with triplet loss and allows more effective mapping of the input into the embedding space since not only pairs or triplets but entire picture is seen at the same time. This approach also allows to get quite good results even without selection of hard pairs for training instead of random ones (as done in [this kernel](https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563)). However, combining those two approaches may further boost the convergence of the network, especially at the later stage of training.\n",
    "\n",
    "Another novel thing I use in this kernel is **training on rectangular images instead of square ones**. After extracting bounding boxes (thanks to [this fork](https://www.kaggle.com/suicaokhoailang/generating-whale-bounding-boxes) and to Martin Piotte for posting the original kernel), the aspect ratio of crops with whale tails is approximetly 3:1. In most public kernels using bounding boxes approach, the produced crops with tails are just squeezed to square images. In this kernel I use 576x192 crops generated based on bounding boxes without stretching. This kernel is written with using fast.ai 0.7 since a newer version of fast.ai doesn't work well in kaggle: using more than one core for data loading leads to [bus error](https://www.kaggle.com/product-feedback/72606) \"DataLoader worker (pid 137) is killed by signal: Bus error\". Therefore, when I tried to write similar kernel with fast.ai 1.0, it appeared to be much slower, more than 1 hour per epoch vs. 20-30 min with this kernel if ResNet34 is used. People interested in fast.ai 1.0 could check an example of Siamese network [here](https://www.kaggle.com/raghavab1992/siamese-with-fast-ai). Another thing, fast.ai 0.7 is not really designed to build Siamese and Triplet networks, therefore some parts are a little bit far away from a standard usage of the library.\n",
    "\n",
    "**Highlights: Multiple (all vs. all) loss, training on rectangular images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#!pip install fastai==0.7.0 --no-deps\n",
    "#!pip install torch==0.4.1 torchvision==0.2.1\n",
    "#!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "9636cfe30b9f697340fd1dc67144d2cd13ff7e6f"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (core.py, line 31)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/dollofcuty/anaconda3/envs/fastaiv07/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3267\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-1-d8f175f4212a>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from fastai.conv_learner import *\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/dollofcuty/anaconda3/envs/fastaiv07/lib/python3.7/site-packages/fastai/conv_learner.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from .core import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/dollofcuty/anaconda3/envs/fastaiv07/lib/python3.7/site-packages/fastai/core.py\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    if cuda: a = to_gpu(a, async=True)\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f89644af8d8a80cbfa272379dc34d18c54c144c7"
   },
   "outputs": [],
   "source": [
    "PATH = Path('./data/')\n",
    "TRAIN = PATH/'train/'\n",
    "TEST = PATH/'test/'\n",
    "LABELS = PATH/'train.csv'\n",
    "BOXES = PATH/'bounding_boxes.csv'\n",
    "MODLE_INIT = PATH/'pytorch-pretrained-models/'\n",
    "\n",
    "n_embedding = 256\n",
    "bs = 16\n",
    "ratio = 3\n",
    "sz0 = 192\n",
    "sz = (ratio*sz0,sz0)\n",
    "nw = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "6735c2010738adcbad39f6934d1031f4bd8f0c62"
   },
   "source": [
    "### Data\n",
    "The class Loader creates crops with sizes 576x192 based on the bounding boxes without stretching the image. In addition, data augmentation based on [imgaug library](https://github.com/aleju/imgaug) is applied. This library is quite interesting in the context of the competition since it supports hue and saturation augmentations as well as conversion to gray scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5088875811058ee7a5c286dbbde3fc68cbe2dfc5"
   },
   "outputs": [],
   "source": [
    "def open_image(fn):\n",
    "    flags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR\n",
    "    if not os.path.exists(fn):\n",
    "        raise OSError('No such file or directory: {}'.format(fn))\n",
    "    elif os.path.isdir(fn):\n",
    "        raise OSError('Is a directory: {}'.format(fn))\n",
    "    else:\n",
    "        try:\n",
    "            im = cv2.imread(str(fn), flags)\n",
    "            if im is None: raise OSError(f'File not recognized by opencv: {fn}')\n",
    "            return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            raise OSError('Error handling image at: {}'.format(fn)) from e\n",
    "\n",
    "class Loader():\n",
    "    def __init__(self,path,tfms_g=None, tfms_px=None):\n",
    "        #tfms_g - geometric augmentation (distortion, small rotation, zoom)\n",
    "        #tfms_px - pixel augmentation and flip\n",
    "        self.boxes = pd.read_csv(BOXES).set_index('Image')\n",
    "        self.path = path\n",
    "        self.tfms_g = iaa.Sequential(tfms_g,random_order=False) \\\n",
    "                        if tfms_g is not None else None\n",
    "        self.tfms_px = iaa.Sequential(tfms_px,random_order=False) \\\n",
    "                        if tfms_px is not None else None\n",
    "    def __call__(self, fname):\n",
    "        fname = os.path.basename(fname)\n",
    "        x0,y0,x1,y1 = tuple(self.boxes.loc[fname,['x0','y0','x1','y1']].tolist())\n",
    "        img = open_image(os.path.join(self.path,fname))\n",
    "        if self.tfms_g != None: img = self.tfms_g.augment_image(img)\n",
    "        l1,l0,_ = img.shape\n",
    "        b0,b1 = x1-x0 + 50, y1-y0 + 20 #add extra paddning\n",
    "        b0n,b1n = (b0, b0/ratio) if b0**2/ratio > b1**2*ratio else (b1*ratio, b1)\n",
    "        if b0n > l0: b0n,b1n = l0,b1n*l0/b0n\n",
    "        if b1n > l1: b0n,b1n = b0n*l1/b1n,l1\n",
    "        x0n = (x0 + x1 - b0n)/2\n",
    "        x1n = (x0 + x1 + b0n)/2\n",
    "        y0n = (y0 + y1 - b1n)/2\n",
    "        y1n = (y0 + y1 + b1n)/2\n",
    "        x0n,x1n,y0n,y1n = int(x0n),int(x1n),int(y0n),int(y1n)\n",
    "        if(x0n < 0): x0n,x1n = 0,x1n-x0n\n",
    "        elif(x1n > l0): x0n,x1n = x0n+l0-x1n,l0\n",
    "        if(y0n < 0): y0n,y1n = 0,y1n-y0n\n",
    "        elif(y1n > l1): y0n,y1n = y0n+l1-y1n,l1\n",
    "        img = cv2.resize(img[y0n:y1n,x0n:x1n,:], sz)\n",
    "        if self.tfms_px != None: img = self.tfms_px.augment_image(img)\n",
    "        return img.astype(np.float)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1d6372c8ea26c641c02df8c12631c6c805d017c"
   },
   "source": [
    "The Dataset class below generates triplets of images: original image, different image with the same label, an image with different label (**including new_label images**). I do not use selection of triplets based on the performance of the network (to focus only on ones that confuse the network the most) since it is compensated by multiple loss function (all vs. all comparison). Out of 100s negative examples in a batch it is quite likely to have several tough ones. However, more careful selection of triplets could slightly improve convergence of the network (though it requres additional computational time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78aeb73926f7707c12dd5fdef74dd1cc4e2d0d9b"
   },
   "outputs": [],
   "source": [
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, data, path, transform):\n",
    "        df = data.copy()\n",
    "        counts = Counter(df.Id.values)\n",
    "        df['c'] = df['Id'].apply(lambda x: counts[x])\n",
    "        #in the production runs df.c>1 should be used\n",
    "        fnames = df[(df.c>2) & (df.Id != 'new_whale')].Image.tolist()\n",
    "        df['label'] = df.Id\n",
    "        df.loc[df.c == 1,'label'] = 'new_whale'\n",
    "        df = df.sort_values(by=['c'])\n",
    "        df.label = pd.factorize(df.label)[0]\n",
    "        l1 = 1 + df.label.max()\n",
    "        l2 = len(df[df.label==0])\n",
    "        df.loc[df.label==0,'label'] = range(l1, l1+l2) #assign unique ids\n",
    "        self.labels = df.copy().set_index('Image')\n",
    "        self.names = df.copy().set_index('label')\n",
    "        if path == TRAIN:\n",
    "            #data augmentation: 8 degree rotation, 10% stratch, shear\n",
    "            tfms_g = [iaa.Affine(rotate=(-8, 8),mode='reflect',\n",
    "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, shear=(-8,8))]\n",
    "            #data augmentation: horizontal flip, hue and staturation augmentation,\n",
    "            #gray scale, blur\n",
    "            tfms_px = [iaa.Fliplr(0.5), iaa.AddToHueAndSaturation((-20, 20)),\n",
    "                iaa.Grayscale(alpha=(0.0, 1.0)),iaa.GaussianBlur((0, 1.0))]\n",
    "            self.loader = Loader(path,tfms_g,tfms_px)\n",
    "        else: self.loader = Loader(path)\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        label = self.labels.loc[self.fnames[i],'label']\n",
    "        #random selection of a positive example\n",
    "        for j in range(10): #sometimes loc call fails\n",
    "            try:\n",
    "                names = self.names.loc[label].Image\n",
    "                break\n",
    "            except: None\n",
    "        name_p = names if isinstance(names,str) else \\\n",
    "            random.sample(set(names) - set([self.fnames[i]]),1)[0]\n",
    "        #random selection of a negative example\n",
    "        for j in range(10): #sometimes loc call fails\n",
    "            try:\n",
    "                names = self.names.loc[self.names.index!=label].Image\n",
    "                break\n",
    "            except: None\n",
    "        name_n = names if isinstance(names,str) else names.sample(1).values[0]\n",
    "        imgs = [self.loader(os.path.join(self.path,self.fnames[i])),\n",
    "                self.loader(os.path.join(self.path,name_p)),\n",
    "                self.loader(os.path.join(self.path,name_n)),\n",
    "                label,label,self.labels.loc[name_n,'label']]\n",
    "        return imgs\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        return 0\n",
    "    \n",
    "    def get(self, tfm, x, y):\n",
    "        if tfm is None:\n",
    "            return (*x,0)\n",
    "        else:\n",
    "            x1, y1 = tfm(x[0],x[3])\n",
    "            x2, y2 = tfm(x[1],x[4])\n",
    "            x3, y3 = tfm(x[2],x[5])\n",
    "            #combine all images into one tensor\n",
    "            x = np.stack((x1,x2,x3),0)\n",
    "            return x,(y1,y2,y3)\n",
    "        \n",
    "    def get_names(self,label):\n",
    "        names = []\n",
    "        for j in range(10):\n",
    "            try:\n",
    "                names = self.names.loc[label].Image\n",
    "                break\n",
    "            except: None\n",
    "        return names\n",
    "        \n",
    "    @property\n",
    "    def is_multi(self): return True\n",
    "    @property\n",
    "    def is_reg(self):return True\n",
    "    \n",
    "    def get_c(self): return n_embedding\n",
    "    def get_n(self): return len(self.fnames)\n",
    "    \n",
    "#class for loading an individual images when embedding is computed\n",
    "class FilesDataset_single(FilesDataset):\n",
    "    def __init__(self, data, path, transform):\n",
    "        self.loader = Loader(path)\n",
    "        fnames = os.listdir(path)\n",
    "        super().__init__(fnames, transform, path)\n",
    "        \n",
    "    def get_x(self, i):\n",
    "        return self.loader(os.path.join(self.path,self.fnames[i]))\n",
    "                           \n",
    "    def get_y(self, i):\n",
    "        return 0\n",
    "        \n",
    "    @property\n",
    "    def is_multi(self): return True\n",
    "    @property\n",
    "    def is_reg(self):return True\n",
    "    \n",
    "    def get_c(self): return n_embedding\n",
    "    def get_n(self): return len(self.fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bf08dcffcd4e4cc0b53e024618720ba5ff2fe08"
   },
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    tfms = tfms_from_model(resnet34, sz, crop_type=CropType.NO)\n",
    "    tfms[0].tfms = [tfms[0].tfms[2],tfms[0].tfms[3]]\n",
    "    tfms[1].tfms = [tfms[1].tfms[2],tfms[1].tfms[3]]\n",
    "    df = pd.read_csv(LABELS)\n",
    "    trn_df, val_df = train_test_split(df,test_size=0.2, random_state=42)\n",
    "    ds = ImageData.get_ds(pdFilesDataset, (trn_df,TRAIN), (val_df,TRAIN), tfms)\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a52a9a47400d36d7449384d1209eff496d4413a"
   },
   "source": [
    "The image below demonstrates an example of triplets of rectangular 576x192 augmented images used for training. To be honest, some of those triplets are quite hard, and I don't think that I could even reach the same performance as the model after training (~99% accuracy in identifications of 2 similar images in a triplet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1bae0892446f49e1939764fb36d6c5ae1ee42e44",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "md = get_data(sz,bs)\n",
    "\n",
    "x,y = next(iter(md.trn_dl))\n",
    "print(x.shape, y[0].shape)\n",
    "\n",
    "def display_imgs(x):\n",
    "    columns = 3\n",
    "    rows = min(bs,16)\n",
    "    fig=plt.figure(figsize=(columns*8, rows*3))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            idx = j+i*columns\n",
    "            fig.add_subplot(rows, columns, idx+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow((x[j][i,:,:,:]*255).astype(np.int))\n",
    "    plt.show()\n",
    "    \n",
    "display_imgs((md.trn_ds.denorm(x[:,0,:,:,:]),md.trn_ds.denorm(x[:,1,:,:,:]),md.trn_ds.denorm(x[:,2,:,:,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c6a947158a6f6596e595a01046cd09ec043ab11"
   },
   "source": [
    "### Model\n",
    "In this kernel I use ResNeXt50 instead of ResNet34 since it gaves slightly better performance after training within kernel time limit: 0.600 vs 0.588. The convolutional part is taken from the original ResNeXt50 model pretrained on ImageNet, meanwhile adaptive pooling allows using of images of any sizes and aspect ratios. On the top, 2 fully connected layers are added to convert the prediction of convolutional part into embedding space. Conversion of the input images into embedding allows quite efficient and robust inference. Also, multiple loss can be applied quite easily.\n",
    "\n",
    "Instead of Euclidean distance in the embedding space, the calculation of the similarity between images can be done with a several layer network as in [this kernel](https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563). This approach could boost the score; however, it would require some modification of the model to allow all vs. all comparison during training. In particular, a copy of the head part of the network with shared weights must be created for each compared pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51bd808549c475d5b5ac483afb7c7e60b3f71667"
   },
   "outputs": [],
   "source": [
    "def resnext50(pretrained=True):\n",
    "    model = resnext_50_32x4d()\n",
    "    name = 'resnext_50_32x4d.pth'\n",
    "    if pretrained:\n",
    "        path = os.path.join(MODLE_INIT,name)\n",
    "        load_model(model, path)\n",
    "    return model\n",
    "\n",
    "class TripletResneXt50(nn.Module):\n",
    "    def __init__(self, pre=True, emb_sz=64, ps=0.5):\n",
    "        super().__init__()\n",
    "        encoder = resnext50(pretrained=pre)\n",
    "        self.cnn = nn.Sequential(encoder[0],encoder[1],nn.ReLU(),encoder[3],\n",
    "                        encoder[4],encoder[5],encoder[6],encoder[7])\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool2d(), Flatten(), nn.Dropout(ps),\n",
    "                        nn.Linear(4096, 512), nn.ReLU(), nn.BatchNorm1d(512),\n",
    "                        nn.Dropout(ps), nn.Linear(512, emb_sz))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1,x2,x3 = x[:,0,:,:,:],x[:,1,:,:,:],x[:,2,:,:,:]\n",
    "        x1 = self.head(self.cnn(x1))\n",
    "        x2 = self.head(self.cnn(x2))\n",
    "        x3 = self.head(self.cnn(x3))\n",
    "        return torch.cat((x1.unsqueeze_(-1),x2.unsqueeze_(-1),x3.unsqueeze_(-1)),dim=-1)\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.head(self.cnn(x))\n",
    "    \n",
    "class ResNeXt50Model():\n",
    "    def __init__(self,pre=True,name='TripletResneXt50',**kwargs):\n",
    "        self.model = to_gpu(TripletResneXt50(pre=True,**kwargs))\n",
    "        self.name = name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        m = self.model.module if isinstance(self.model,FP16) else self.model\n",
    "        if precompute:\n",
    "            return [m.head]\n",
    "        c = children(m.cnn)\n",
    "        return list(split_by_idxs(c,[5])) + [m.head]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "588ff9ef22ef09568e371794cdaf9387d05a0fc2"
   },
   "source": [
    "### Loss function\n",
    "I my tests I have performed comparison of several loss functions and found that contrastive loss works the best in the current setup. Distance based logistic loss gives similar performance when model is trained with singe precision, but worse results for training with half precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5887f278eaac38400954d53b1cfa80c5a6a88159"
   },
   "outputs": [],
   "source": [
    "def Contrastive_loss(preds, target, size_average=True, m=10.0):\n",
    "    #matrix of all vs all comparisons\n",
    "    t = torch.cat(target)\n",
    "    sz = t.shape[0]\n",
    "    t1 = t.unsqueeze(1).expand((sz,sz))\n",
    "    t2 = t1.transpose(0,1)\n",
    "    y = t1==t2\n",
    "    \n",
    "    pred = torch.cat((preds[:,:,0], preds[:,:,1], preds[:,:,2]))\n",
    "    half = True if isinstance(pred,torch.cuda.HalfTensor) else False\n",
    "    if half : pred = pred.float()\n",
    "    pred1 = pred.unsqueeze(1).expand((sz,sz,-1))\n",
    "    pred2 = pred1.transpose(0,1)\n",
    "    d = (pred1 - pred2).pow(2).sum(dim=-1)\n",
    "    loss_p = d[y==1]\n",
    "    loss_n = F.relu(m - torch.sqrt(d[y==0]))**2\n",
    "    loss = torch.cat((loss_p,loss_n),0)\n",
    "    loss = loss.mean() if size_average else loss.sum()\n",
    "    if half : pred = pred.half()\n",
    "    return loss\n",
    "\n",
    "def DB_acc(preds, target):\n",
    "    v, p, n = preds[:,:,0], preds[:,:,1], preds[:,:,2]\n",
    "    dp = (p - v).pow(2).sum(dim=1)\n",
    "    dn = (n - v).pow(2).sum(dim=1)\n",
    "    return (dp < dn).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b6f2d53d1efb2a1252843960ece5a769d55701e"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "9fbb63f57b1ad3f40c85cee43ef90c14c5e4fad2"
   },
   "outputs": [],
   "source": [
    "learner = ConvLearner(md,ResNeXt50Model(ps=0.0,emb_sz=n_embedding))\n",
    "learner.opt_fn = optim.Adam\n",
    "learner.clip = 1.0 #gradient clipping\n",
    "learner.crit = Contrastive_loss\n",
    "learner.metrics = [DB_acc]\n",
    "learner #click \"output\" to see details of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e45e02522cc0bdae1bbbbfd6825949cf35178b0"
   },
   "source": [
    "I begin with finding the optimal learning rate. The following function runs training with different learning rate and records the loss. Increase of the loss indicates onset of divergence of training. The optimal lr lies in the vicinity of the minimum of the curve but before the onset of divergence. Based on the following plot, for the current setup the divergence starts at ~5e-3, and the recommended learning rate is ~5e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2ef1d4dcb3a8a4f4e4040bc8d9e8a749f364214"
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    learner.lr_find()\n",
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f8d094e319e86f2a2ae032c6f20ef3a42c802ce"
   },
   "source": [
    "First, I train only the fully connected part of the model while keeping the rest frozen. It allows to avoid corruption of the pretrained weights at the initial stage of training due to random initialization of the head layers. So the power of transfer learning is fully utilized when the training is continued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31cf2eea892ab56280f7ada4db72aff9ccfd46ab"
   },
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    learner.fit(lr,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "afc18d43b9da9b0315973df29d633a84d26666e8"
   },
   "source": [
    "Next, I unfreeze all weights and allow training of entire model. One trick that I use is applying different learning rates in different parts of the model: the learning rate in the fully connected part is still lr, last two blocks of ResNeXt are trained with lr/5, and first layers are trained with lr/25. Since low-level detectors do not vary much from one image data set to another, the first layers do not require substantial retraining compared to the parts of the model working with high level features. Another trick is learning rate annealing. Periodic learning rate increase followed by slow decrease drives the system out of steep minima (when lr is high) towards broader ones (which are explored when lr decreases) that enhances the ability of the model to generalize and reduces overfitting. The length of the cycles gradually increases during training. Usage of half precision doubles the maximum batch size that allows to compare more pairs in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9c2d0bcc53b6cdf01f9d7c8e10ec573aea8f85b"
   },
   "outputs": [],
   "source": [
    "learner.unfreeze()\n",
    "lrs=np.array([lr/25,lr/5,lr])\n",
    "learner.half() #half precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d50ef8f1b26bced8872933422f26abfeaa32908f"
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    learner.fit(lrs/4,4,cycle_len=1,use_clr=(10,20))\n",
    "    learner.fit(lrs/8,2,cycle_len=2,use_clr=(10,20))\n",
    "    learner.fit(lrs/16,1,cycle_len=2,use_clr=(10,20)) \n",
    "learner.save('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f5648fc035df75b0a371c05fbf83aa377e4d8fc"
   },
   "source": [
    "### Embedding\n",
    "The following code converts images into embedding vectors which are used later to generate predictions based on the nearest neighbor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ca98b9da1eee10c8f61efe03ea3d4fb23b6b796"
   },
   "outputs": [],
   "source": [
    "def extract_embedding(model,path):\n",
    "    tfms = tfms_from_model(resnet34, sz, crop_type=CropType.NO)\n",
    "    tfms[0].tfms = [tfms[0].tfms[2],tfms[0].tfms[3]]\n",
    "    tfms[1].tfms = [tfms[1].tfms[2],tfms[1].tfms[3]]\n",
    "    ds = ImageData.get_ds(FilesDataset_single, (None,TRAIN), (None,TRAIN),\n",
    "         tfms, test=(None,path))\n",
    "    md = ImageData(PATH, ds, 3*bs, num_workers=nw, classes=None)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.zeros((len(md.test_dl.dataset), n_embedding))\n",
    "        start=0\n",
    "        for i, (x, y) in enumerate(md.test_dl, start=0):\n",
    "            size = x.shape[0]\n",
    "            if isinstance(model,FP16):\n",
    "                preds[start:start+size,:] = model.module.get_embedding(x.half())\n",
    "            else:\n",
    "                preds[start:start+size,:] = model.get_embedding(x)\n",
    "            start+= size\n",
    "        return preds, [os.path.basename(name) for name in md.test_dl.dataset.fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccf3cd5fd576aca25e7bd2bcda779b4e6d34bd97"
   },
   "outputs": [],
   "source": [
    "emb, names = extract_embedding(learner.model,TRAIN)\n",
    "df = pd.DataFrame({'files':names,'emb':emb.tolist()})\n",
    "df.emb = df.emb.map(lambda emb: ' '.join(list([str(i) for i in emb])))\n",
    "df.to_csv('train_emb.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d84df614cff6768fd236c0c49e5223457463ec49"
   },
   "outputs": [],
   "source": [
    "emb, names = extract_embedding(learner.model,TEST)\n",
    "df = pd.DataFrame({'files':names,'emb':emb.tolist()})\n",
    "df.emb = df.emb.map(lambda emb: ' '.join(list([str(i) for i in emb])))\n",
    "df.to_csv('test_emb.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e11ae99694ba3210eec298d94988487e23bca1cd"
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7676339611fc3374bd47fa69cab31d0fe9700a8f"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(LABELS).set_index('Image')\n",
    "trn_emb = pd.read_csv(os.path.join('../working/','train_emb.csv'))\n",
    "trn_emb['emb'] = [[float(i) for i in s.split()] for s in trn_emb['emb']]\n",
    "trn_emb.set_index('files',inplace=True)\n",
    "train_df = data.join(trn_emb)\n",
    "train_df = train_df.reset_index()\n",
    "train_preds = np.array(train_df.emb.tolist())\n",
    "#the split should be the same as one used for training.\n",
    "trn_df, val_df = train_test_split(train_df,test_size=0.2, random_state=42)\n",
    "trn_preds = np.array(trn_df.emb.tolist())\n",
    "val_preds = np.array(val_df.emb.tolist())\n",
    "trn_df = trn_df.reset_index()\n",
    "val_df = val_df.reset_index()\n",
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c639f4cb8de3764b70a69561d2edf9b66771748"
   },
   "source": [
    "Find 16 nearest train neighbors in embedding space for each validation image. Since there can be several neighbors with the same label, instead of 5 I use 16 here. The following code will select 5 nearest neighbors with different labels. \"new_whale\" label can be assigned as a prediction at a distance dcut. In this case, if the number of neighbors at a distance shorter than dcut is less than 5, the image is considered to be different from others, and \"new_whale\" is assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "501f146dfbe08ad06b30ad2c34b3a63e5f7851cd"
   },
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=16)\n",
    "neigh.fit(trn_preds)\n",
    "distances_trn, neighbors_trn = neigh.kneighbors(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0024f5a83c56c6b26b5805c1c386334e969a5d10"
   },
   "outputs": [],
   "source": [
    "def get_nlabels_trn(idx:int,trn_df,test_df,dcut):\n",
    "    l0 = test_df.loc[idx].Id\n",
    "    nbs = dict()\n",
    "    for i in range(0,16):\n",
    "        nb = neighbors_trn[idx,i]\n",
    "        l, d = trn_df.loc[nb].Id, distances_trn[idx,i]\n",
    "        if d > dcut and 'new_whale' not in nbs: nbs['new_whale'] = dcut\n",
    "        if l not in nbs: nbs[l] = d\n",
    "        if len(nbs) >= 5: break\n",
    "    nbs_sorted = sorted(nbs.items(), key=lambda kv: kv[1])\n",
    "    score = 0.0\n",
    "    for i in range(min(len(nbs_sorted),5)):\n",
    "        if nbs_sorted[i][0] == l0:\n",
    "            score = 1.0/(i + 1.0)\n",
    "            break\n",
    "    return l0, nbs_sorted, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "217f43317208d9d12244e571e521a0715d0d89d3"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "dcut = 3.75\n",
    "for idx in val_df.index:\n",
    "    _,_,s = get_nlabels_trn(idx,trn_df,val_df,dcut)\n",
    "    scores.append(s)\n",
    "print(np.array(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ee2d1fa46d07a49cdc665e6ca15888896637a7e"
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f703d13b4875c9225acb07c44f8857328f23d124"
   },
   "outputs": [],
   "source": [
    "test_emb = pd.read_csv(os.path.join('../working/','test_emb.csv'))\n",
    "test_emb['emb'] = [[float(i) for i in s.split()] for s in test_emb['emb']]\n",
    "test_emb.set_index('files',inplace=True)\n",
    "test_df = test_emb.reset_index()\n",
    "test_preds = np.array(test_df.emb.tolist())\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "459c651519d457bb4f00044179edf834a6764de0"
   },
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=16)\n",
    "neigh.fit(train_preds)\n",
    "distances_test, neighbors_test = neigh.kneighbors(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "015ce690817986f6b16f4a3a80e9286ef6492353"
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    nbs = dict()\n",
    "    for i in range(0,16):\n",
    "        nb = neighbors_test[idx,i]\n",
    "        l, d = train_df.loc[nb].Id, distances_test[idx,i]\n",
    "        if d > dcut and 'new_whale' not in nbs: nbs['new_whale'] = dcut\n",
    "        if l not in nbs: nbs[l] = d\n",
    "        if len(nbs) >= 5: break\n",
    "    nbs_sorted = sorted(nbs.items(), key=lambda kv: kv[1])\n",
    "    p = ' '.join([lb[0] for lb in nbs_sorted])\n",
    "    pred.append({'Image':row.files,'Id':p})\n",
    "pd.DataFrame(pred).to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
